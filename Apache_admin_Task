							*********************************************
									Admin tasks
							*********************************************
#################################################################################################################################################################
							
							-----------******* Safemode ******----------------

hdfs dfsadmin -safemode get
hdfs dfsadmin -safemode enter

hdfs dfsadmin -safemode leave
hdfs dfsadmin -safemode wait

################################################################################################################################################################

					-----**** Creating a data pipeline from s3 to hdfs and vice versa ****-----

--------------------------------------------------------NOTE----------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------
We havr create pipeline connection EC2 to S3 so we use two way using access key credintial Id or 2nd way IMRoles. most user access key.
whay we config on Core-site.xml -->>>> because configuration apply on all cluster 
[[[[[[[[[[[[[access.key avilable on AWS user_Name(kale)-->My_security_Credentioal--->>>>Access_key & Secret Key]]]]]]]]]]]]]]]]]]]
---------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------



--------------------------------------------------------NOTE----------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------
All require library functions are avilable in CLASS_PATH.
All hadoop cluster is take this required packages from this location
By default not availabe because not Every hadoop cluster not connect to S3 its depent on project
---------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------

cd /usr/local/hadoop/etc/hadoop


nano core-site.xml

<property>
  <name>fs.s3a.access.key</name>
  <description>AWS access key ID used by S3A file system.</description>
  <value>AKIAW3WWW43UVE47UJUM</value> 
</property>

<property>
  <name>fs.s3a.secret.key</name>
  <description>AKIAW3WWW43UVE47UJUM</description>
  <value>s7I41d61/OacMF77m6bmqcko18BI3tSgHOT5GLwf</value>
</property>




AWSAccessKeyId= AKIAW3WWW43UVE47UJUM
AWSSecretKey=   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


nano hadoop-env.sh

export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*

cd

hadoop fs -cp s3n://cpamol/image001.jpg hdfs:///user/hduser/                   
                                                       -------------------outdated not use              
hdfs dfs -cp s3a://cpamol/IMG_20191109_161825_2.jpg /user/hduser/
	                                                ----------------------its new
################################################################################################################################################################
							-----*** Distributed copy ***-----

nano core-site.xml

<property>
<name>fs.s3.impl</name>
<value>org.apache.hadoop.fs.s3.S3FileSystem</value>
<description>The FileSystem for s3a: S3 uris.</description>
</property>



nano mapred-site.xml
         ------------------------------because map task run for copy
<property>
  <!-- Add to the classpath used when running an M/R job -->
  <name>mapreduce.application.classpath</name>
  <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/tools/lib/*</value>
</property>


hadoop distcp /user/hduser/abc s3a://cpamol/

hadoop distcp hdfs:///user/hduser/jinga.xml s3a://<bucket-name>/

##############################################################################################################################################################

							-----***** Creating snapshots ****----

hdfs dfsadmin -allowSnapshot /user/hduser/

hdfs dfs -createSnapshot /user/hduser/ snapshot1

hdfs lsSnapshottableDir        
				--------------------------This cmd is show which directory's snapshort has been allow

hdfs dfs -ls /user/hduser/.snapshot

hdfs dfs -deleteSnapshot /user/hduser/ snapshot1

hdfs dfsadmin -disallowSnapshot /user/hduser/

--> check snapshots on NN web UI (50070 / snpshort)

###################################################################################################################################################################

							-----**** Hadoop Archivals ****-----


--------------------------------------------------------NOTE----------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------
Hadoop archivals is not use in currently
but we know it and We can do it
Archivals is done only on already proccessed data not on running data.
i.e it done on thos data thos are not in use, because we can not delete any data on clusetr
if we want to read archive data then we need to extract It 
---------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------


hadoop archive -archiveName xyz.har -p /user/hduser abc hadoop-2.8.2.tar.gz /user/hduser

hdfs dfs -ls

hdfs dfs -ls -R xyz.har             ------------------this for show whats Mapreduce operations perform

hdfs dfs -ls -R har:///user/hduser/xyz.har
                                   -------------------its show which filed_Dir avilable in  .har file


hdfs dfs -mkdir /user/hduser/unarch
				-------------------create dir for extract

hdfs dfs -cp har:///user/hduser/xyz.har/abc hdfs:/user/hduser/unarch

hdfs dfs -ls -R /user/hduser/unarch


############################################################################################################################################################

						-----**** Change ownerships ****----- 

	[hdfs is underline OS but its require group or user present on operating system]

hdfs dfs -chgrp sysadmin /data
                              --------------------/data is on hdfs location  sysyadmin is group
hdfs dfs -chown hduser /data

hdfs dfs -chown hduser:hadoop /data
                          ---------------change owner:group at same time

hdfs dfs -chmod 700 data

hdfs dfs -ls -R

##############################################################################################################################################################

						-----**** Configuring the HDFS quota ****-----

hdfs dfsadmin -setQuota 20 /user/ubuntu
					-----------------set file quota 20 files only we calculate with replication also 

hdfs dfsadmin -clrQuota /user/ubuntu



hdfs dfsadmin -setSpaceQuota /user/ubuntu
					-----------------suppose we have set space quota 10gb we calculate with replication also & asign 30gb 

hdfs dfsadmin -clrSpaceQuota /user/ubuntu

hdfs dfs -count -q /path/to/directory

quota  rem_quota  space_quota  rem_space_quota  dir_count  file_count  size  file
 none     inf        54975	    56897           3           8     786955 /u/ 


############################################################################################################################################################

							-----*** Namenode metadata backup ****-----

hdfs fsck /
				--------------check system healthy or not
hdfs dfsadmin -report

hdfs dfsadmin -report > report.txt
				---------------genrate on local system	hduser	
							
	 																
hdfs dfsadmin -safemode enter

hdfs dfsadmin -saveNamespace
	--------------------savemanespace it means maunal check pointing & merge olde edits &FSimage and create new Edits $FSimage

hdfs namenode -finalize
			----------finalize means refresh 
cd /usr/local/hadoop/data/hdfs/namenode/

cp -r current/ /home/ubuntu/meta-bup


----->>>> Leave safemode
hdfs dfsadmin -safemode leave

##############################################################################################################################################################

								-----**** Decommisioning nodes ****-----

cd /usr/local/hadoop/etc/hadoop
nano excludes
<private-dns-dn>

nano hdfs-site.xml
<property>
<name>dfs.hosts.exclude</name>
<value>/usr/local/hadoop/etc/hadoop/excludes</value>
<final>true</final>
</property>

nano yarn-site.xml
<property>
<name>yarn.resourcemanager.nodes.exclude-path</name>
<value>/usr/local/hadoop/etc/hadoop/excludes</value>
</property>

ssh rm
cd /usr/local/hadoop/etc/hadoop
nano excludes
<private-dns-dn>

nano yarn-site.xml
<property>
<name>yarn.resourcemanager.nodes.exclude-path</name>
<value>/usr/local/hadoop/etc/hadoop/excludes</value>
</property>

exit to NN

hdfs dfsadmin -refreshNodes

ssh rm
yarn rmadmin -refreshNodes
exit

hdfs dfsadmin -report

--> check NN web UI

############################################################################################################################################################3
							-----**** Filesystem check ****----

hdfs fsck <path>
hdfs fsck /

## Options for fsck. To be given after path
-delete					Delete corrupted files.
-files					Print out files being checked.
-files -blocks				Print out the block report
-files -blocks -locations		Print out locations for every block.
-includeSnapshots			Include snapshot data if the given path indicates a snapshottable directory or there are snapshottable 						directories under it.
-list-corruptfileblocks			Print out list of missing blocks and files they belong to.
-move					Move corrupted files to /lost+found.
-blockId				Print out information about the block.

##############################################################################################################################################################
							-----**** Trash configuration ****-----


cd /usr/local/hadoop/etc/hadoop/
nano core-site.xml

property>
<name>fs.trash.interval</name>
<value>30</value>
<description>Number of minutes between trash checkpoints. If zero, the trash feature is disabled</description>
</property>
<property>
<name>fs.trash.checkpoint.interval</name>
<value>15</value>
</property>

cd
hdfs dfs -rm <file>

hdfs dfs -ls -R <path>

## To permanently delete without going to trash
 hdfs dfs -rm -skipTrash <path-to-file>

###############################################################################################################################################################
							-----**** Changing the block size ****-----
cd /usr/local/hadoop/etc/hadoop/

nano hdfs-site.xml

<property>
<name>dfs.block.size</name>
<value>256m</value> 
</property>

cd

hadoop version                  ---------------------------its use in cloudera .cloudera not show we can get from this command

hadoop fs -du hdfs:/  -------------disk usage show without riplication


hadoop fs -count hdfs:/ --------- file count show without riplication

###############################################################################################################################################################
							----**** Setting Replication for a dataset ----*****

hdfs dfs -setrep -w 4 <file-name>


##############################################################################################################################################################
							-----**** Yarn Administration ****----

## Submit inbuild hadoop jobs (Open 4 terminals and submit same job at a time)


yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar pi  4 10000

yarn application -list

yarn application -list -appStates ALL
           ---------------------------show all details of job runnig or not

##To get application ID use 

yarn application -list

yarn application -status application_1459542433815_0002


#################################################################################################################################################################
							----------***** Change the priority *****--------
------------------------------[change priority befor starting job or running job]


mapred job -set-priority pplication_1459542433815_0002 <priority>

priority--> VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW


#################################################################################################################################################################
							----------********* Kill an application *********-----------------

yarn application -kill application_1459542433815_0002

yarn logs -applicationId application_1583212331513_0003	

yarn application -list -appStates ALL
           ---------------------------show all details of job runnig or not

###############################################################################################################################################################
								-----**** Commisioning new nodes ****-----

cd /usr/local/hadoop/etc/hadoop
nano includes
<private-dns-dn>

nano hdfs-site.xml
<property>
<name>dfs.hosts.include</name>
<value>/usr/local/hadoop/etc/hadoop/includes</value>
<final>true</final>
</property>

nano yarn-site.xml
<property>
<name>yarn.resourcemanager.nodes.include-path</name>
<value>/usr/local/hadoop/etc/hadoop/includes</value>
</property>

ssh rm
cd /usr/local/hadoop/etc/hadoop
nano includes
<private-dns-dn>

nano yarn-site.xml
<property>
<name>yarn.resourcemanager.nodes.include-path</name>
<value>/usr/local/hadoop/etc/hadoop/includes</value>
</property>

exit to NN

hdfs dfsadmin -refreshNodes

ssh rm
yarn rmadmin -refreshNodes
exit

hdfs dfsadmin -report

--> check NN web UI
